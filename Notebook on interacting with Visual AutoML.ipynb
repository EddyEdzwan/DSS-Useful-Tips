{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "creator": "admin",
    "createdOn": 1663213700546,
    "tags": [],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.7.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a sample notebook that was created to show how a user could interact programmatically with the AutoML Visual Interface in Dataiku DSS. Most of the content was pieced together from articles within Dataiku\u0027s [Knowledge Base](https://doc.dataiku.com/)\n\nThough not extensive, I hope this notebook exposes you to some of the key functions that you may need to use and also some useful functions that might be relevant when trying to set these tasks to run automatically through [Scenarios](https://doc.dataiku.com/dss/latest/scenarios/index.html)\n\nFor more information on the topic and the APIs, I would recommend to refer to [this](https://doc.dataiku.com/dss/latest/python-api/ml.html#obtaining-a-handle-to-an-existing-ml-task)\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "execution_count": 62,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport datetime\n\n# # from outside DSS\n# import dataikuapi"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Establishing connection to DSS Instance"
      ]
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# establish connection to instance\nclient \u003d dataiku.api_client()\n\n# # from outside DSS\n# host \u003d \"http://localhost:11200\"\n# apiKey \u003d \"BCtZV0kLIxHAWCPZTtZM8vgbj2Yzst9F\"\n# client \u003d dataikuapi.DSSClient(host, apiKey)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieving Project and ML Tasks"
      ]
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get project in DSS\nproj \u003d client.get_project(\"CCFRAUDAVDCORESTART\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 5,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# # List ML task that has been created for project\n# proj.list_ml_tasks()\n\n# Get ML task that has been created\nml_task \u003d proj.get_ml_task(\"IryrkUVj\", \"BA05g5C8\") # provide visual_analysis_id \u0026 ml_task_id which can be found in the URL\n\n\n# # Otherwise, you can also create your own ML training task --\n# # Create a new ML Task to predict the variable \"target\" from \"trainset\"\n# ml_task \u003d p.create_prediction_ml_task( # use .create_clustering_ml_task() for clustering tasks\n#     input_dataset\u003d\"trainset\",\n#     target_variable\u003d\"target\",\n#     ml_backend_type\u003d\u0027PY_MEMORY\u0027, # ML backend to use\n#     guess_policy\u003d\u0027DEFAULT\u0027 # Template to use for setting default parameters\n# )\n\n# # Wait for the ML task to be ready\n# ml_task.wait_guess_complete()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Changing settings within ML Tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Retrieve settings"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "settings \u003d ml_task.get_settings()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Selection"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "settings.reject_feature(\"not_useful\")\nsettings.use_feature(\"useful\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature handling"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use impact coding rather than dummy-coding\nfs \u003d settings.get_feature_preprocessing(\"mycategory\")\nfs[\"category_handling\"] \u003d \"IMPACT\"\n\n# Impute missing with most frequent value\nfs[\"missing_handling\"] \u003d \"IMPUTE\"\nfs[\"missing_impute_with\"] \u003d \"MODE\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Algorithm selection"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "settings.set_algorithm_enabled(\"GBT_CLASSIFICATION\", True)\n# use .get_all_possible_algorithm_names() to find the str names of algorithms"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Algorithm-specific hyperparameter tuning"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rf_settings \u003d settings.get_algorithm_settings(\"RANDOM_FOREST_CLASSIFICATION\")\n\n# rf_settings is an object representing the settings for this algorithm.\n# The \u0027enabled\u0027 attribute indicates whether this algorithm will be trained.\n# Other attributes are the various hyperparameters of the algorithm.\n\n# The precise hyperparameters for each algorithm are not all documented, so let\u0027s\n# print the dictionary keys to see available hyperparameters.\n# Alternatively, tab completion will provide relevant hints to available hyperparameters.\nprint(rf_settings.keys())\n\n# Let\u0027s first have a look at rf_settings.n_estimators which is a multi-valued hyperparameter\n# represented as a NumericalHyperparameterSettings object\nprint(rf_settings.n_estimators)\n\n# Set multiple explicit values for \"n_estimators\" to be explored during the search\nrf_settings.n_estimators.definition_mode \u003d \"EXPLICIT\"\nrf_settings.n_estimators.values \u003d [100, 200]\n# Alternatively use the set_values setter\nrf_settings.n_estimators.set_values([100, 200])\n\n# Set a range of values for \"n_estimators\" to be explored during the search\nrf_settings.n_estimators.definition_mode \u003d \"RANGE\"\nrf_settings.n_estimators.range.min \u003d 10\nrf_settings.n_estimators.range.max \u003d 100\nrf_settings.n_estimators.range.nb_values \u003d 5  # Only relevant for grid-search\n# Alternatively, use the set_range setter\nrf_settings.n_estimators.set_range(min\u003d10, max\u003d100, nb_values\u003d5)\n\n# Let\u0027s now have a look at rf_settings.selection_mode which is a single-valued hyperparameter\n# represented as a SingleCategoryHyperparameterSettings object.\n# The object stores the valid options for this hyperparameter.\nprint(rf_settings.selection_mode)\n\n# Features selection mode is not multi-valued so it\u0027s not actually searched during the\n# hyperparameter search\nrf_settings.selection_mode \u003d \"sqrt\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Saving changes made to settings"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "settings.save()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Begin training session"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_time \u003d datetime.datetime.strftime(datetime.datetime.now(), \u0027%Y/%m/%d %H:%M\u0027)\nml_task.start_train(session_name\u003df\u0027Programmatic Run @ {training_time}\u0027)\nml_task.wait_train_complete()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interact with training session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Get latest training session id"
      ]
    },
    {
      "execution_count": 43,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "latest_session_id \u003d sorted(ml_task.get_trained_models_ids(),reverse\u003dTrue)[0].split(\u0027-\u0027)[4]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Compare results of latest training session \u0026 store best performing model"
      ]
    },
    {
      "execution_count": 56,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get the identifiers of the trained models\n# There will be a list of the models is multiple models were trained in the same session\nids \u003d ml_task.get_trained_models_ids(session_id\u003dlatest_session_id)\n\nalgorithm \u003d \u0027\u0027\nalgorithm_id \u003d \u0027\u0027\nbest_auc \u003d 0\n\n# Iterate through the trained models and get the best performing algo\nfor index, id in enumerate(ids):\n    details \u003d ml_task.get_trained_model_details(id)\n    \n    if index \u003d\u003d 0:\n        algorithm \u003d details.get_modeling_settings()[\"algorithm\"]\n        algorithm_id \u003d id\n        best_auc \u003d details.get_performance_metrics()[\"auc\"]\n        \n    if details.get_performance_metrics()[\"auc\"] \u003e best_auc:\n        algorithm \u003d details.get_modeling_settings()[\"algorithm\"]\n        algorithm_id \u003d id\n        best_auc \u003d details.get_performance_metrics()[\"auc\"]\n        \nprint(f\u0027\u0027\u0027\nBest Performing Model : {algorithm}\nMetric Achieved : {best_auc}\n\u0027\u0027\u0027)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "\nBest Performing Model : LIGHTGBM_CLASSIFICATION\nMetric Achieved : 0.7358589241485487\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Deploy Best Model in Flow"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# # Deploying best model as a new Saved Model in the flow\n# ret \u003d ml_task.deploy_to_flow(algorithm_id, \"my_model\", \"trainset\")\n\n# print(\"Deployed to saved model id \u003d %s train recipe \u003d %s\" % (ret[\"savedModelId\"], ret[\"trainRecipeName\"]))\n\n# Redeploy best model to an exising Saved Model in the flow\nsaved_model_id \u003d proj.list_saved_models()[0][\u0027id\u0027] # Assuming there is only one Saved Model in the flow\nml_task.redeploy_to_flow(algorithm_id, saved_model_id\u003dsaved_model_id)"
      ],
      "outputs": []
    }
  ]
}